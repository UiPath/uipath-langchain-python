{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_command(cmd):\n",
    "    process = subprocess.Popen(\n",
    "        cmd.split(),\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        encoding='utf-8',\n",
    "        errors='replace',\n",
    "        bufsize=1\n",
    "    )\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    process.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B2B Logistics Email Processing Agent\n",
    "\n",
    "This agent automatically processes customer emails for a logistics company. It validates companies, classifies emails (orders, complaints, inquiries), calculates pricing with dynamic discounts based on order history, creates queue items for audit and further processing with department routing, and generates professional email responses.\n",
    "\n",
    "## Uses:\n",
    "\n",
    "- data fabric for data storage\n",
    "- 2 indexes for performing RAG: one containing the company policy and one containing shipment requirements\n",
    "- a queue for audit and further processing\n",
    "- an escalation app for Human In The Loop (in case the agent is not confident enough in its resolution)\n",
    "- an mcp server\n",
    "\n",
    "All of those are available through the UiPath python sdks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate to UiPath\n",
    "\n",
    "Authenticate to the UiPath staging environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Authenticating with UiPath ...\n",
      "âœ“  Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "run_command('uipath auth --staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Agent\n",
    "\n",
    "Generate agent configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â ‹ Initializing UiPath project ...\n",
      "'uipath.json' already exists, skipping.\n",
      "'bindings.json' already exists, skipping.\n",
      "âœ“  Created 'entry-points.json' file with 1 entrypoint(s).\n",
      "âœ“  Created 1 mermaid diagram file(s).\n",
      "âœ“  Updated: CLAUDE.md, CLI_REFERENCE.md, SDK_REFERENCE.md, AGENTS.md, REQUIRED_STRUCTURE.md\n"
     ]
    }
   ],
   "source": [
    "run_command('uipath init')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Terminal\n",
    "\n",
    "Install the uipath-dev dependency and open an interactive development terminal for agent development and testing.\n",
    "\n",
    "```bash\n",
    "uv add uipath-dev --dev\n",
    "uipath dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging the Agent\n",
    "\n",
    "Run the agent with a sample input for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "uipath debug agent --input-file sample_inputs\\input-complaint.json --output-file output.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to StudioWeb\n",
    "\n",
    "Push local sources, evaluations, and bindings to StudioWeb for development and production use.\n",
    "\n",
    "First, set the project ID for the remote StudioWeb project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added UIPATH_PROJECT_ID=9bd0dbfb-13fd-40d1-b4d5-de0c92b23d65 to .env\n"
     ]
    }
   ],
   "source": [
    "project_env_var = 'UIPATH_PROJECT_ID=9bd0dbfb-13fd-40d1-b4d5-de0c92b23d65'\n",
    "\n",
    "with open('.env', 'a') as f:\n",
    "    f.write(f'\\n{project_env_var}\\n')\n",
    "\n",
    "print(f\"Added {project_env_var} to .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing UiPath project to Studio Web...\n",
      "Uploading 'agent.mermaid'\n",
      "Uploading 'AGENTS.md'\n",
      "Uploading 'bindings.json'\n",
      "Uploading 'CLAUDE.md'\n",
      "Updating 'entry-points.json'\n",
      "Uploading 'langgraph.json'\n",
      "Uploading 'output.json'\n",
      "Updating 'pyproject.toml'\n",
      "Uploading 'README.md'\n",
      "Uploading 'uipath.json'\n",
      "Uploading 'uv.lock'\n",
      "Updating 'evaluation-set-default.json'\n",
      "Uploading 'discount-evaluator.json'\n",
      "Uploading 'discount-evaluator_copy.json'\n",
      "File 'evaluator-default.json' is up to date\n",
      "Uploading 'exact-match-1766006646266.json'\n",
      "Uploading 'llm-judge-output-1765995809662.json'\n",
      "Uploading 'llm-judge-trajectory-1765871655357.json'\n",
      "Uploading 'tool-call-order-1765871126253.json'\n",
      "Uploading 'discount.py'\n",
      "Uploading 'discount_copy.py'\n",
      "Uploading 'discount-evaluator-types.json'\n",
      "Uploading 'input-complaint.json'\n",
      "Uploading 'input-complaint.md'\n",
      "Uploading 'input-customer-question.json'\n",
      "Uploading 'input-customer-question.md'\n",
      "Uploading 'input-low-confidence.json'\n",
      "Uploading 'input-low-confidence.md'\n",
      "Uploading 'input-order.json'\n",
      "Uploading 'input-order.md'\n",
      "Uploading 'input-unclear.json'\n",
      "Uploading 'input-unclear.md'\n",
      "Uploading 'entities.py'\n",
      "Uploading 'main.py'\n",
      "Uploading 'middlewares.py'\n",
      "Uploading 'models.py'\n",
      "Uploading 'prompts.py'\n",
      "Uploading '.uipath/studio_metadata.json'\n",
      "\n",
      "Importing referenced resources to Studio Web project...\n",
      "âœ“  Created reference for resource: company_policy (kind = Index, type = None)\n",
      "âœ“  Created reference for resource: Shipment_index_dev (kind = Index, type = None)\n",
      "âœ“  Created reference for resource: SimpleApprovalApp (kind = app, type = Workflow Action)\n",
      "\n",
      " ðŸ”µ Resource import summary: 3 total resources - 3 created, 0 updated, 0 unchanged, 0 not found\n"
     ]
    }
   ],
   "source": [
    "run_command('uipath push')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering Issue\n",
    "\n",
    "The order processing uses a `validate_shipment_capacity` tool that validates shipments based on weight category.\n",
    "\n",
    "### Problematic Tool Description (Current)\n",
    "\n",
    "```python\n",
    "@tool(\"validate_shipment_capacity\")\n",
    "def validate_shipment_capacity(weight_category: str) -> dict:\n",
    "    \"\"\"Validate if the shipment can be processed based on weight category.\n",
    "\n",
    "    Critical validation step for shipment processing. Call this tool to verify shipment eligibility.\n",
    "    If validation fails, stop further processing immediately.\n",
    "\n",
    "    Args:\n",
    "        weight_category: Weight classification (Light, Medium, Heavy)\n",
    "\n",
    "    Returns:\n",
    "        Validation result with status and message\n",
    "    \"\"\"\n",
    "    # Always returns: {\"validated\": True, \"message\": \"Shipment capacity validated successfully\"}\n",
    "```\n",
    "\n",
    "**Problematic Prompt Section:**\n",
    "\n",
    "```\n",
    "### Shipment Validation\n",
    "Use the validate_shipment_capacity tool to verify shipment eligibility. If validation fails, stop processing immediately.\n",
    "```\n",
    "\n",
    "**Issue:** Prompt says to validate but doesn't specify WHEN. Tool description says \"Critical validation step.\" Agent calls it early without knowing the actual weight category from the shipment data.\n",
    "\n",
    "**Why it works by chance:** Tool hardcodes `validated: True`, so validation always passes regardless of when it's called. Agent gets lucky - validation succeeds even with wrong/missing weight info.\n",
    "\n",
    "**Why it's wrong:** Agent calls validation before querying the index for actual shipment details. It validates with a guessed or default weight category instead of the real one.\n",
    "\n",
    "### Correct Prompt Addition\n",
    "\n",
    "```\n",
    "### Shipment Validation\n",
    "After retrieving shipment details from the knowledge base and identifying the weight category,\n",
    "use validate_shipment_capacity to verify shipment eligibility. If validation fails, stop processing.\n",
    "```\n",
    "\n",
    "This demonstrates how tools that always succeed can hide incorrect execution order, creating false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the prompt to specify correct validation order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Updated prompt with correct validation order\n"
     ]
    }
   ],
   "source": [
    "with open('src/prompts.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "old_validation = \"Use the validate_shipment_capacity tool to verify shipment eligibility. If validation fails, stop processing immediately.\"\n",
    "\n",
    "new_validation = \"\"\"Call validate_shipment_capacity ONLY AFTER you get the output from shipment_retriever_tool\"\"\"\n",
    "\n",
    "content = content.replace(old_validation, new_validation)\n",
    "\n",
    "with open('src/prompts.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"âœ“ Updated prompt with correct validation order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "Validate new prompt with evaluation sets.\n",
    "\n",
    "```bash\n",
    "uipath eval agent --output-file output.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Evaluator for Discount Calculation\n",
    "\n",
    "For the discount calculation scenario, out-of-the-box evaluators aren't sufficient. We need a custom evaluator that validates the discount logic based on order history.\n",
    "\n",
    "Create a new custom evaluator for discount validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“  Created new evaluator: evaluations/evaluators/custom/discount.py\n",
      "ðŸ’¡ Next steps:\n",
      "ðŸ’¡   1. Edit evaluations/evaluators/custom/discount.py to implement your evaluation logic\n",
      "ðŸ’¡   2. Run uipath register evaluator discount.py to generate the evaluator spec\n"
     ]
    }
   ],
   "source": [
    "run_command('uipath add evaluator discount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Copied custom evaluator code to evaluations/discount.py\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy the custom evaluator code\n",
    "shutil.copy('evaluations/evaluators/custom/discount_copy.py', 'evaluations/evaluators/custom/discount.py')\n",
    "print(\"âœ“ Copied custom evaluator code to evaluations/discount.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found custom evaluator file: evals/evaluators/custom/discount.py\n",
      "Found custom evaluator class: DiscountEvaluator\n",
      "âœ“  Generated evaluator types: evaluations/evaluators/custom/types/discount-evaluator-types.json\n",
      "âœ“  Generated evaluator spec: evals/evaluators/discount-evaluator.json\n"
     ]
    }
   ],
   "source": [
    "run_command('uipath register evaluator discount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Copied custom evaluator configuration to evals/discount-evaluator.json\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Copy the custom evaluator JSON configuration\n",
    "shutil.copy('evaluations/evaluators/discount-evaluator_copy.json', 'evaluations/evaluators//discount-evaluator.json')\n",
    "print(\"âœ“ Copied custom evaluator configuration to evals/discount-evaluator.json\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "run_command('uipath push')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geeks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
